{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN06lO1c0W++tEe0Q0lo/Dw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeraintWong/f20aa_cw2/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('train_processed.csv')"
      ],
      "metadata": {
        "id": "9C8mpVDm9iHX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XoOMSrLHiASV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(str(text))\n",
        "\n",
        "train_df['Text_Processed_Tokens'] = train_df['Text_Processed'].apply(tokenize_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDt8lbt9978A",
        "outputId": "b84cf3dd-f51c-40ad-f734-0f0df434a0a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to pass into N-gram and Vector Space\n",
        "X = train_df['Text_Processed'].tolist()\n",
        "y = [score - 1 for score in train_df['Score'].tolist()]\n",
        "\n",
        "# Used to pass into word embeddings\n",
        "X_token = train_df['Text_Processed_Tokens']\n",
        "\n",
        "# Splitting the dataset to 80/20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "classifiers1 = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
        "    \"Naïve Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    \"Logistic Regression\": {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__solver': ['liblinear']\n",
        "    },\n",
        "    \"Decision Tree\": {\n",
        "        'classifier__max_depth': [5, 10, 20],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4],  # Minimum samples required at each leaf node\n",
        "        'classifier__criterion': ['gini', 'entropy']  # The function to measure quality of a split\n",
        "    },\n",
        "    \"Naïve Bayes\": {\n",
        "        'classifier__priors': [None, [0.5, 0.5]],  # Example for prior probabilities\n",
        "        'classifier__var_smoothing': [1e-9, 1e-8]  # Tuning the variance smoothing parameter\n",
        "    }\n",
        "}\n",
        "\n",
        "results = []"
      ],
      "metadata": {
        "id": "BC8Zn3q098eX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "word2vec = api.load(\"word2vec-google-news-300\")\n",
        "glove_vectors = api.load(\"glove-twitter-25\")\n"
      ],
      "metadata": {
        "id": "JN_4hLpEDh0B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert text to word embeddings\n",
        "def get_embedding(tokens, model):\n",
        "    \"\"\"\n",
        "    Generates an average word embedding for a list of tokens.\n",
        "    \"\"\"\n",
        "    word_vectors = [model[word] for word in tokens if word in model]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Convert text data to embeddings\n",
        "X_word2vec = np.array([get_embedding(text, word2vec) for text in X_token])\n",
        "X_glove = np.array([get_embedding(text, glove_vectors) for text in X_token])\n",
        "\n",
        "subset_fraction = 0.1  # 10% of the data for experimentation\n",
        "\n",
        "# Train-test split for word embeddings\n",
        "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_word2vec, y, test_size=0.2, random_state=42)\n",
        "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(X_glove, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Selecting the subset of data (10% of the training data)\n",
        "X_train_w2v_subset = X_train_w2v[:int(subset_fraction * len(X_train_w2v))]\n",
        "y_train_w2v_subset = y_train_w2v[:int(subset_fraction * len(y_train_w2v))]\n",
        "\n",
        "X_train_glove_subset = X_train_glove[:int(subset_fraction * len(X_train_glove))]\n",
        "y_train_glove_subset = y_train_glove[:int(subset_fraction * len(y_train_glove))]\n",
        "\n",
        "# Results container\n",
        "results = []\n",
        "\n",
        "# Test classifiers on word embeddings\n",
        "for clf_name, clf in classifiers1.items():\n",
        "    print(f\"\\n=== {clf_name} ===\")\n",
        "\n",
        "    # Word2Vec\n",
        "    pipeline_w2v = Pipeline([\n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "    param_grid_w2v = param_grids.get(clf_name, {})\n",
        "\n",
        "    if param_grid_w2v:\n",
        "        search_w2v = RandomizedSearchCV(\n",
        "            pipeline_w2v,\n",
        "            param_distributions=param_grid_w2v,\n",
        "            n_iter=5,\n",
        "            cv=3,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        search_w2v.fit(X_train_w2v_subset, y_train_w2v_subset)  # Use the subset for fitting\n",
        "        best_model_w2v = search_w2v.best_estimator_\n",
        "        print(f\"Best Parameters for Word2Vec: {search_w2v.best_params_}\")  # Show best hyperparameters\n",
        "    else:\n",
        "        pipeline_w2v.fit(X_train_w2v_subset, y_train_w2v_subset)  # Use the subset for fitting\n",
        "        best_model_w2v = pipeline_w2v\n",
        "\n",
        "    y_pred_w2v = best_model_w2v.predict(X_test_w2v)\n",
        "    accuracy_w2v = accuracy_score(y_test_w2v, y_pred_w2v)\n",
        "    precision_w2v = precision_score(y_test_w2v, y_pred_w2v, average='weighted', zero_division=1)\n",
        "    recall_w2v = recall_score(y_test_w2v, y_pred_w2v, average='weighted', zero_division=1)\n",
        "    f1_w2v = f1_score(y_test_w2v, y_pred_w2v, average='weighted', zero_division=1)\n",
        "\n",
        "    print(\"Word2Vec\")\n",
        "    print(classification_report(y_test_w2v, y_pred_w2v, zero_division=1))\n",
        "\n",
        "    results.append([\"Word Embedding\", clf_name, \"Word2Vec\", accuracy_w2v, precision_w2v, recall_w2v, f1_w2v])\n",
        "\n",
        "    # GloVe\n",
        "    pipeline_glove = Pipeline([\n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "    param_grid_glove = param_grids.get(clf_name, {})\n",
        "\n",
        "    if param_grid_glove:\n",
        "        search_glove = RandomizedSearchCV(\n",
        "            pipeline_glove,\n",
        "            param_distributions=param_grid_glove,\n",
        "            n_iter=5,\n",
        "            cv=3,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "        search_glove.fit(X_train_glove_subset, y_train_glove_subset)  # Use the subset for fitting\n",
        "        best_model_glove = search_glove.best_estimator_\n",
        "        print(f\"Best Parameters for GloVe: {search_glove.best_params_}\")  # Show best hyperparameters\n",
        "    else:\n",
        "        pipeline_glove.fit(X_train_glove_subset, y_train_glove_subset)  # Use the subset for fitting\n",
        "        best_model_glove = pipeline_glove\n",
        "\n",
        "    y_pred_glove = best_model_glove.predict(X_test_glove)\n",
        "    accuracy_glove = accuracy_score(y_test_glove, y_pred_glove)\n",
        "    precision_glove = precision_score(y_test_glove, y_pred_glove, average='weighted', zero_division=1)\n",
        "    recall_glove = recall_score(y_test_glove, y_pred_glove, average='weighted', zero_division=1)\n",
        "    f1_glove = f1_score(y_test_glove, y_pred_glove, average='weighted', zero_division=1)\n",
        "\n",
        "    print(\"GloVe\")\n",
        "    print(classification_report(y_test_glove, y_pred_glove, zero_division=1))\n",
        "\n",
        "    results.append([\"Word Embedding\", clf_name, \"GloVe\", accuracy_glove, precision_glove, recall_glove, f1_glove])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TPlCBEG-DxY",
        "outputId": "ee10a2d9-422a-465e-f07e-1440a524e881"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for Word2Vec: {'classifier__solver': 'liblinear', 'classifier__C': 10}\n",
            "Word2Vec\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.45      0.50      5644\n",
            "           1       0.27      0.03      0.05      3214\n",
            "           2       0.33      0.08      0.13      4679\n",
            "           3       0.35      0.09      0.15      8688\n",
            "           4       0.71      0.97      0.82     39602\n",
            "\n",
            "    accuracy                           0.68     61827\n",
            "   macro avg       0.45      0.32      0.33     61827\n",
            "weighted avg       0.60      0.68      0.60     61827\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for GloVe: {'classifier__solver': 'liblinear', 'classifier__C': 10}\n",
            "GloVe\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.15      0.22      5644\n",
            "           1       0.43      0.00      0.00      3214\n",
            "           2       0.19      0.01      0.01      4679\n",
            "           3       0.22      0.01      0.01      8688\n",
            "           4       0.66      0.98      0.79     39602\n",
            "\n",
            "    accuracy                           0.65     61827\n",
            "   macro avg       0.38      0.23      0.21     61827\n",
            "weighted avg       0.53      0.65      0.53     61827\n",
            "\n",
            "\n",
            "=== Naïve Bayes ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "6 fits failed out of a total of 12.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "6 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\", line 266, in fit\n",
            "    return self._partial_fit(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\", line 452, in _partial_fit\n",
            "    raise ValueError(\"Number of priors must match number of classes.\")\n",
            "ValueError: Number of priors must match number of classes.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.41682186 0.41682186        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for Word2Vec: {'classifier__var_smoothing': 1e-09, 'classifier__priors': None}\n",
            "Word2Vec\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.51      0.33      5644\n",
            "           1       0.11      0.25      0.16      3214\n",
            "           2       0.16      0.26      0.20      4679\n",
            "           3       0.19      0.26      0.22      8688\n",
            "           4       0.81      0.48      0.60     39602\n",
            "\n",
            "    accuracy                           0.42     61827\n",
            "   macro avg       0.30      0.35      0.30     61827\n",
            "weighted avg       0.59      0.42      0.47     61827\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "6 fits failed out of a total of 12.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "6 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\", line 266, in fit\n",
            "    return self._partial_fit(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/naive_bayes.py\", line 452, in _partial_fit\n",
            "    raise ValueError(\"Number of priors must match number of classes.\")\n",
            "ValueError: Number of priors must match number of classes.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.59708875 0.59708875        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for GloVe: {'classifier__var_smoothing': 1e-09, 'classifier__priors': None}\n",
            "GloVe\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.39      0.31      5644\n",
            "           1       0.13      0.05      0.07      3214\n",
            "           2       0.18      0.05      0.08      4679\n",
            "           3       0.25      0.04      0.07      8688\n",
            "           4       0.70      0.87      0.78     39602\n",
            "\n",
            "    accuracy                           0.61     61827\n",
            "   macro avg       0.30      0.28      0.26     61827\n",
            "weighted avg       0.53      0.61      0.55     61827\n",
            "\n",
            "\n",
            "=== Decision Tree ===\n",
            "Best Parameters for Word2Vec: {'classifier__min_samples_split': 5, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': 5, 'classifier__criterion': 'gini'}\n",
            "Word2Vec\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.05      0.08      5644\n",
            "           1       1.00      0.00      0.00      3214\n",
            "           2       1.00      0.00      0.00      4679\n",
            "           3       1.00      0.00      0.00      8688\n",
            "           4       0.65      0.99      0.78     39602\n",
            "\n",
            "    accuracy                           0.64     61827\n",
            "   macro avg       0.79      0.21      0.17     61827\n",
            "weighted avg       0.71      0.64      0.51     61827\n",
            "\n",
            "Best Parameters for GloVe: {'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 2, 'classifier__max_depth': 5, 'classifier__criterion': 'gini'}\n",
            "GloVe\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.02      0.04      5644\n",
            "           1       1.00      0.00      0.00      3214\n",
            "           2       1.00      0.00      0.00      4679\n",
            "           3       1.00      0.00      0.00      8688\n",
            "           4       0.64      1.00      0.78     39602\n",
            "\n",
            "    accuracy                           0.64     61827\n",
            "   macro avg       0.81      0.20      0.16     61827\n",
            "weighted avg       0.72      0.64      0.50     61827\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(solver='liblinear', C=10, max_iter=500)"
      ],
      "metadata": {
        "id": "ufn6cZ-x7oFC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_full = Pipeline([\n",
        "    ('classifier', clf)\n",
        "])\n",
        "\n",
        "# Train the model on the entire training dataset\n",
        "pipeline_full.fit(X_train_w2v, y_train_w2v)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_full = pipeline_full.predict(X_test_w2v)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy_full = accuracy_score(y_test_w2v, y_pred_full)\n",
        "print(f\"Accuracy: {accuracy_full:.4f}\")\n",
        "\n",
        "# Print the classification report for more details\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_w2v, y_pred_full, zero_division=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQaoC6rYJnTO",
        "outputId": "d64c7319-d9ce-4fd1-ecc2-05032b9d84e3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6833\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.48      0.52      5644\n",
            "           1       0.37      0.02      0.04      3214\n",
            "           2       0.35      0.08      0.13      4679\n",
            "           3       0.36      0.09      0.14      8688\n",
            "           4       0.71      0.97      0.82     39602\n",
            "\n",
            "    accuracy                           0.68     61827\n",
            "   macro avg       0.47      0.33      0.33     61827\n",
            "weighted avg       0.61      0.68      0.61     61827\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ookqCPjjJ9Ke"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}