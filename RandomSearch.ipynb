{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11230080,"sourceType":"datasetVersion","datasetId":7014557},{"sourceId":11266192,"sourceType":"datasetVersion","datasetId":7042223},{"sourceId":11278121,"sourceType":"datasetVersion","datasetId":7050928},{"sourceId":11278125,"sourceType":"datasetVersion","datasetId":7050932}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:57:13.182747Z","iopub.execute_input":"2025-04-04T17:57:13.183056Z","iopub.status.idle":"2025-04-04T17:57:39.029634Z","shell.execute_reply.started":"2025-04-04T17:57:13.183023Z","shell.execute_reply":"2025-04-04T17:57:39.028983Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# Install dependencies as needed:\n# pip install kagglehub[pandas-datasets]\nimport kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n\n# Set the path to the file you'd like to load\nfile_path = \"train_processed.csv\"\n\n# Load the latest version\ntrain_df = kagglehub.load_dataset(\n  KaggleDatasetAdapter.PANDAS,\n  \"buddycyph/train-processed\",\n  file_path,\n  # Provide any additional arguments like \n  # sql_query or pandas_kwargs. See the \n  # documenation for more information:\n  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n)\n\nprint(\"First 5 records:\", train_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:13:29.274861Z","iopub.execute_input":"2025-04-04T19:13:29.275205Z","iopub.status.idle":"2025-04-04T19:13:31.890672Z","shell.execute_reply.started":"2025-04-04T19:13:29.275177Z","shell.execute_reply":"2025-04-04T19:13:31.889729Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-31-05ff9d98374a>:10: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n  train_df = kagglehub.load_dataset(\n","output_type":"stream"},{"name":"stdout","text":"First 5 records:    Score                                               Text  \\\n0      5  I received this product early from the seller!...   \n1      5  *****<br />Numi's Collection Assortment Melang...   \n2      5  I was very careful not to overcook this pasta,...   \n3      5  Buying this multi-pack I was misled by the pic...   \n4      5  These bars are so good! I loved them warmed up...   \n\n                                      Text_Processed  \n0  receive product early seller tastey great midd...  \n1  br numis collection assortment melange include...  \n2  careful overcook pasta make sure take bite eve...  \n3  buying multipack mislead picture whole hazel n...  \n4  bar good love warm definitely think great snac...  \n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau \nimport keras\n\n# Adjust scores to be 0-indexed\ntrain_df['Score'] = train_df['Score'] - 1\n\n# Optional: view class distribution\nprint(\"Class Distribution:\")\nprint(train_df['Score'].value_counts())\n\n# Text and Labels\ntexts = train_df['Text_Processed'].values\nlabels = train_df['Score'].values\ny = to_categorical(labels, num_classes=5)\n\n# Tokenization for Word2Vec\ntokenized_texts = [word_tokenize(text) for text in texts]\n\n# Train Word2Vec model\nw2v_dim = 100\nw2v_model = Word2Vec(sentences=tokenized_texts, vector_size=w2v_dim, window=5, min_count=2, workers=4)\n\n# Tokenization for Keras\nmax_words = 8000\nmax_len = 100\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"\")\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nX_pad = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n\n# Build Embedding Matrix\nvocab_size = min(max_words, len(word_index) + 1)\nembedding_matrix = np.zeros((vocab_size, w2v_dim))\n\nfor word, i in word_index.items():\n    if i >= max_words:\n        continue\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:13:39.249230Z","iopub.execute_input":"2025-04-04T19:13:39.249518Z","iopub.status.idle":"2025-04-04T19:15:48.897504Z","shell.execute_reply.started":"2025-04-04T19:13:39.249495Z","shell.execute_reply":"2025-04-04T19:15:48.896730Z"}},"outputs":[{"name":"stdout","text":"Class Distribution:\nScore\n4    197151\n3     43876\n0     28521\n2     23296\n1     16287\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import keras_tuner as kt\n\n# Define the model-building function\ndef build_model(hp):\n    model = Sequential([\n        Embedding(\n            input_dim=vocab_size, \n            output_dim=w2v_dim, \n            weights=[embedding_matrix],\n            input_length=max_len,\n            trainable=True  # Keep Word2Vec embeddings fixed\n        ),\n        SpatialDropout1D(0.3),\n        Bidirectional(LSTM(\n            hp.Int('units_1', min_value=32, max_value=128, step=32), \n            return_sequences=True, \n            kernel_regularizer=l2(0.001)\n        )),\n        BatchNormalization(),\n        Bidirectional(LSTM(\n            hp.Int('units_2', min_value=32, max_value=128, step=32),\n            kernel_regularizer=l2(0.001)\n        )),\n        Dropout(hp.Float('dropout', min_value=0.4, max_value=0.6, step=0.05)),\n        Dense(hp.Int('dense_units', min_value=16, max_value=64, step=16), activation='relu', kernel_regularizer=l2(0.001)),\n        Dense(5, activation='softmax')\n    ])\n    \n    # Compile the model with a learning rate that is also tunable\n    model.compile(\n        optimizer=keras.optimizers.RMSprop(learning_rate=hp.Float('lr', min_value=1e-6, max_value=1e-2, sampling='log')),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# Instantiate the tuner\ntuner = kt.RandomSearch(\n    build_model, \n    objective='val_accuracy', \n    max_trials=5,  # Number of trials to run\n    executions_per_trial=1,  # Number of models to train per trial\n    directory='tuner_results',\n    project_name='hyperparameter_tuning'\n)\n\n# Perform the search for the best hyperparameters\ntuner.search(X_train, y_train, \n             epochs=10, \n             batch_size=64, \n             validation_data=(X_test, y_test), \n             callbacks=[early_stopping, lr_scheduler])\n\n# Get the best model and hyperparameters\nbest_model = tuner.get_best_models(1)[0]\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n\n# Print best hyperparameters\nprint(\"Best Hyperparameters:\")\nprint(best_hyperparameters.values)\n\n# Evaluate the best model\nloss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:15:48.898730Z","iopub.execute_input":"2025-04-04T19:15:48.899008Z","iopub.status.idle":"2025-04-04T20:13:55.547385Z","shell.execute_reply.started":"2025-04-04T19:15:48.898971Z","shell.execute_reply":"2025-04-04T20:13:55.546233Z"}},"outputs":[{"name":"stdout","text":"Trial 5 Complete [00h 11m 54s]\nval_accuracy: 0.7257508635520935\n\nBest val_accuracy So Far: 0.7257508635520935\nTotal elapsed time: 00h 58m 04s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 20 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-56764e2925f0>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Get the best model and hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mbest_hyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Print best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: BaseTuner.get_best_hyperparameters() got an unexpected keyword argument 'num_models'"],"ename":"TypeError","evalue":"BaseTuner.get_best_hyperparameters() got an unexpected keyword argument 'num_models'","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"file_path = \"sampleSubmission.csv\"\n\n# Load the latest version\nsample_submission = kagglehub.load_dataset(\n  KaggleDatasetAdapter.PANDAS,\n  \"buddycyph/samplesubmission\",\n  file_path,\n  # Provide any additional arguments like \n  # sql_query or pandas_kwargs. See the \n  # documenation for more information:\n  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:22:50.618453Z","iopub.execute_input":"2025-04-04T18:22:50.618781Z","iopub.status.idle":"2025-04-04T18:22:50.813064Z","shell.execute_reply.started":"2025-04-04T18:22:50.618756Z","shell.execute_reply":"2025-04-04T18:22:50.812236Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-a7175d73b4df>:4: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n  sample_submission = kagglehub.load_dataset(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"file_path = \"test.csv\"\n\n# Load the latest version\ntest_df = kagglehub.load_dataset(\n  KaggleDatasetAdapter.PANDAS,\n  \"buddycyph/testdataset\",\n  file_path,\n  # Provide any additional arguments like \n  # sql_query or pandas_kwargs. See the \n  # documenation for more information:\n  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:24:08.427710Z","iopub.execute_input":"2025-04-04T18:24:08.428065Z","iopub.status.idle":"2025-04-04T18:24:09.122177Z","shell.execute_reply.started":"2025-04-04T18:24:08.428034Z","shell.execute_reply":"2025-04-04T18:24:09.121170Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-10-45bd6c1e7e78>:4: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n  test_df = kagglehub.load_dataset(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"test_texts = test_df['Text'].values\ntest_sequences = tokenizer.texts_to_sequences(test_texts)\nX_test_pad = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:27:57.242611Z","iopub.execute_input":"2025-04-04T18:27:57.242935Z","iopub.status.idle":"2025-04-04T18:28:02.019899Z","shell.execute_reply.started":"2025-04-04T18:27:57.242911Z","shell.execute_reply":"2025-04-04T18:28:02.018913Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"y_pred_proba = model.predict(X_test_pad)\ny_pred = np.argmax(y_pred_proba, axis=1)\ny_pred = y_pred + 1 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:28:02.021146Z","iopub.execute_input":"2025-04-04T18:28:02.021466Z","iopub.status.idle":"2025-04-04T18:28:18.381692Z","shell.execute_reply.started":"2025-04-04T18:28:02.021424Z","shell.execute_reply":"2025-04-04T18:28:18.380685Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m3740/3740\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"sample_submission['Score'] = y_pred\n\n# 7. Save to CSV for Kaggle submission\nsample_submission.to_csv(\"john_cena.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:28:32.365209Z","iopub.execute_input":"2025-04-04T18:28:32.365541Z","iopub.status.idle":"2025-04-04T18:28:32.450125Z","shell.execute_reply.started":"2025-04-04T18:28:32.365512Z","shell.execute_reply":"2025-04-04T18:28:32.449477Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}