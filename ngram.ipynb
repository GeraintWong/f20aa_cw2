{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeraintWong/f20aa_cw2/blob/main/ngram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cPOS3i1ZPq8C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('train_processed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "8EBxDOmxzkuk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to pass into N-gram and Vector Space\n",
        "X = train_df['Text_Processed'].tolist()\n",
        "y = [score - 1 for score in train_df['Score'].tolist()]\n",
        "\n",
        "# Splitting the dataset to 80/20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_subset, _, y_train_subset, _ = train_test_split(X_train, y_train, train_size=0.1, random_state=42)\n",
        "\n",
        "# You can also limit the subset of the test data if needed\n",
        "X_test_subset, _, y_test_subset, _ = train_test_split(X_test, y_test, train_size=0.1, random_state=42)\n",
        "\n",
        "classifiers = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
        "    \"Naïve Bayes\": MultinomialNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    \"Logistic Regression\": {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__solver': ['liblinear']\n",
        "    },\n",
        "    \"Decision Tree\": {\n",
        "        'classifier__max_depth': [5, 10, 20],  # Depth of the tree\n",
        "        'classifier__min_samples_split': [2, 5, 10],  # Minimum samples to split an internal node\n",
        "        'classifier__min_samples_leaf': [1, 2, 4],  # Minimum samples required at each leaf node\n",
        "        'classifier__criterion': ['gini', 'entropy']  # The function to measure quality of a split\n",
        "    },\n",
        "    \"Naïve Bayes\": {\n",
        "        'classifier__alpha': [0.1, 1.0, 10.0],  # for Laplace smoothing\n",
        "        'classifier__fit_prior': [True, False]  # whether to learn class prior probabilities\n",
        "    },\n",
        "}\n",
        "\n",
        "results = []"
      ],
      "metadata": {
        "id": "T9B-oNekfPKX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_models = {\n",
        "    \"Unigram\": CountVectorizer(ngram_range=(1, 1)),\n",
        "    \"Bigram\": CountVectorizer(ngram_range=(2, 2)),\n",
        "    \"Trigram\": CountVectorizer(ngram_range=(3, 3)),\n",
        "    \"Combined N-Grams\": CountVectorizer(ngram_range=(1, 3))\n",
        "}"
      ],
      "metadata": {
        "id": "IwoCi8MrfyGK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"\\n=== {clf_name} ===\")\n",
        "\n",
        "    for ngram_name, vectorizer in ngram_models.items():\n",
        "        print(f\"-- {ngram_name} --\")\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', clf)\n",
        "        ])\n",
        "\n",
        "        param_grid = param_grids[clf_name]\n",
        "\n",
        "        if param_grid:\n",
        "            # Perform RandomizedSearchCV\n",
        "            search = RandomizedSearchCV(\n",
        "                pipeline,\n",
        "                param_distributions=param_grid,\n",
        "                n_iter=5,\n",
        "                cv=3,\n",
        "                scoring='accuracy',\n",
        "                n_jobs=-1,\n",
        "                verbose=0\n",
        "            )\n",
        "            search.fit(X_train_subset, y_train_subset)\n",
        "            best_model = search.best_estimator_\n",
        "\n",
        "            # Print best hyperparameters\n",
        "            print(f\"Best Hyperparameters: {search.best_params_}\")\n",
        "        else:\n",
        "            pipeline.fit(X_train_subset, y_train_subset)\n",
        "            best_model = pipeline\n",
        "\n",
        "        # Evaluate the model\n",
        "        y_pred = best_model.predict(X_test_subset)\n",
        "\n",
        "        accuracy = accuracy_score(y_test_subset, y_pred)\n",
        "        precision = precision_score(y_test_subset, y_pred, average='weighted', zero_division=1)\n",
        "        recall = recall_score(y_test_subset, y_pred, average='weighted', zero_division=1)\n",
        "        f1 = f1_score(y_test_subset, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "        # Print evaluation results\n",
        "        print(classification_report(y_test_subset, y_pred, zero_division=1))\n",
        "\n",
        "        # Store results\n",
        "        results.append([\"N-Gram\", clf_name, ngram_name, accuracy, precision, recall, f1])\n"
      ],
      "metadata": {
        "id": "rk3vgIqOfzTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c436a0-b9b6-4eb6-f548-c753d5867335"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression ===\n",
            "-- Unigram --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'classifier__solver': 'liblinear', 'classifier__C': 0.1}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.47      0.51       534\n",
            "           1       0.28      0.07      0.11       323\n",
            "           2       0.38      0.14      0.20       463\n",
            "           3       0.40      0.19      0.25       870\n",
            "           4       0.75      0.95      0.84      3992\n",
            "\n",
            "    accuracy                           0.70      6182\n",
            "   macro avg       0.47      0.36      0.38      6182\n",
            "weighted avg       0.63      0.70      0.64      6182\n",
            "\n",
            "-- Bigram --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'classifier__solver': 'liblinear', 'classifier__C': 10}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.24      0.33       534\n",
            "           1       0.25      0.03      0.06       323\n",
            "           2       0.40      0.09      0.14       463\n",
            "           3       0.38      0.15      0.21       870\n",
            "           4       0.70      0.96      0.81      3992\n",
            "\n",
            "    accuracy                           0.67      6182\n",
            "   macro avg       0.46      0.29      0.31      6182\n",
            "weighted avg       0.60      0.67      0.60      6182\n",
            "\n",
            "-- Trigram --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'classifier__solver': 'liblinear', 'classifier__C': 10}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.01      0.03       534\n",
            "           1       1.00      0.00      0.01       323\n",
            "           2       0.33      0.00      0.00       463\n",
            "           3       0.59      0.01      0.03       870\n",
            "           4       0.65      1.00      0.79      3992\n",
            "\n",
            "    accuracy                           0.65      6182\n",
            "   macro avg       0.69      0.21      0.17      6182\n",
            "weighted avg       0.65      0.65      0.51      6182\n",
            "\n",
            "-- Combined N-Grams --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'classifier__solver': 'liblinear', 'classifier__C': 1}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.50      0.53       534\n",
            "           1       0.33      0.10      0.15       323\n",
            "           2       0.37      0.14      0.21       463\n",
            "           3       0.40      0.20      0.27       870\n",
            "           4       0.76      0.95      0.84      3992\n",
            "\n",
            "    accuracy                           0.70      6182\n",
            "   macro avg       0.48      0.38      0.40      6182\n",
            "weighted avg       0.64      0.70      0.65      6182\n",
            "\n",
            "\n",
            "=== Naïve Bayes ===\n",
            "-- Unigram --\n",
            "Best Hyperparameters: {'classifier__fit_prior': False, 'classifier__alpha': 1.0}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.41      0.48       534\n",
            "           1       0.31      0.02      0.03       323\n",
            "           2       0.21      0.04      0.07       463\n",
            "           3       0.28      0.21      0.24       870\n",
            "           4       0.74      0.93      0.82      3992\n",
            "\n",
            "    accuracy                           0.67      6182\n",
            "   macro avg       0.42      0.32      0.33      6182\n",
            "weighted avg       0.60      0.67      0.61      6182\n",
            "\n",
            "-- Bigram --\n",
            "Best Hyperparameters: {'classifier__fit_prior': True, 'classifier__alpha': 1.0}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.11      0.18       534\n",
            "           1       0.25      0.00      0.01       323\n",
            "           2       0.53      0.02      0.03       463\n",
            "           3       0.28      0.03      0.06       870\n",
            "           4       0.66      0.99      0.79      3992\n",
            "\n",
            "    accuracy                           0.66      6182\n",
            "   macro avg       0.48      0.23      0.22      6182\n",
            "weighted avg       0.58      0.66      0.54      6182\n",
            "\n",
            "-- Trigram --\n",
            "Best Hyperparameters: {'classifier__fit_prior': True, 'classifier__alpha': 1.0}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.07      0.13       534\n",
            "           1       0.50      0.01      0.02       323\n",
            "           2       0.26      0.03      0.05       463\n",
            "           3       0.35      0.08      0.13       870\n",
            "           4       0.67      0.98      0.79      3992\n",
            "\n",
            "    accuracy                           0.65      6182\n",
            "   macro avg       0.46      0.23      0.23      6182\n",
            "weighted avg       0.57      0.65      0.55      6182\n",
            "\n",
            "-- Combined N-Grams --\n",
            "Best Hyperparameters: {'classifier__fit_prior': True, 'classifier__alpha': 0.1}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.32      0.42       534\n",
            "           1       0.18      0.02      0.03       323\n",
            "           2       0.29      0.07      0.11       463\n",
            "           3       0.30      0.15      0.20       870\n",
            "           4       0.72      0.96      0.82      3992\n",
            "\n",
            "    accuracy                           0.67      6182\n",
            "   macro avg       0.42      0.30      0.32      6182\n",
            "weighted avg       0.59      0.67      0.60      6182\n",
            "\n",
            "\n",
            "=== Decision Tree ===\n",
            "-- Unigram --\n",
            "Best Hyperparameters: {'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 4, 'classifier__max_depth': 5, 'classifier__criterion': 'gini'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.16      0.22       534\n",
            "           1       0.11      0.00      0.01       323\n",
            "           2       1.00      0.00      0.00       463\n",
            "           3       0.31      0.02      0.03       870\n",
            "           4       0.66      0.98      0.79      3992\n",
            "\n",
            "    accuracy                           0.65      6182\n",
            "   macro avg       0.49      0.23      0.21      6182\n",
            "weighted avg       0.58      0.65      0.53      6182\n",
            "\n",
            "-- Bigram --\n",
            "Best Hyperparameters: {'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 2, 'classifier__max_depth': 20, 'classifier__criterion': 'gini'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.10      0.16       534\n",
            "           1       0.25      0.01      0.01       323\n",
            "           2       0.31      0.01      0.02       463\n",
            "           3       0.45      0.07      0.12       870\n",
            "           4       0.66      0.98      0.79      3992\n",
            "\n",
            "    accuracy                           0.66      6182\n",
            "   macro avg       0.45      0.23      0.22      6182\n",
            "weighted avg       0.58      0.66      0.55      6182\n",
            "\n",
            "-- Trigram --\n",
            "Best Hyperparameters: {'classifier__min_samples_split': 5, 'classifier__min_samples_leaf': 4, 'classifier__max_depth': 20, 'classifier__criterion': 'entropy'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.03      0.05       534\n",
            "           1       0.27      0.01      0.02       323\n",
            "           2       1.00      0.01      0.03       463\n",
            "           3       0.56      0.02      0.03       870\n",
            "           4       0.65      1.00      0.79      3992\n",
            "\n",
            "    accuracy                           0.65      6182\n",
            "   macro avg       0.62      0.21      0.18      6182\n",
            "weighted avg       0.64      0.65      0.52      6182\n",
            "\n",
            "-- Combined N-Grams --\n",
            "Best Hyperparameters: {'classifier__min_samples_split': 5, 'classifier__min_samples_leaf': 2, 'classifier__max_depth': 5, 'classifier__criterion': 'entropy'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.11      0.16       534\n",
            "           1       0.33      0.01      0.01       323\n",
            "           2       0.27      0.01      0.01       463\n",
            "           3       1.00      0.00      0.00       870\n",
            "           4       0.66      0.98      0.79      3992\n",
            "\n",
            "    accuracy                           0.65      6182\n",
            "   macro avg       0.51      0.22      0.20      6182\n",
            "weighted avg       0.63      0.65      0.53      6182\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(solver='liblinear', C=1, max_iter=500)\n",
        "\n",
        "# Create a pipeline with the classifier\n",
        "pipeline_full = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', C=1, max_iter=500))\n",
        "])\n",
        "\n",
        "# Train the model on the full training dataset\n",
        "pipeline_full.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_full = pipeline_full.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy on Test Set: {accuracy_full:.4f}\")\n",
        "\n",
        "# Print the classification report for more detailed evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_full, zero_division=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2gzm-HzcmM7",
        "outputId": "230d3653-d7d0-492b-cc3d-303c871b8951"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test Set: 0.7278\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.64      0.65      5644\n",
            "           1       0.40      0.14      0.21      3214\n",
            "           2       0.44      0.25      0.31      4679\n",
            "           3       0.45      0.24      0.31      8688\n",
            "           4       0.79      0.95      0.86     39602\n",
            "\n",
            "    accuracy                           0.73     61827\n",
            "   macro avg       0.55      0.44      0.47     61827\n",
            "weighted avg       0.68      0.73      0.69     61827\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kgb4f6L3gEMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpMq/U18tQqSvpDqpHehq0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}