{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11266192,"sourceType":"datasetVersion","datasetId":7042223}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies as needed:\n# pip install kagglehub[pandas-datasets]\nimport kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n\n# Set the path to the file you'd like to load\nfile_path = \"train_processed.csv\"\n\n# Load the latest version\ntrain_df = kagglehub.load_dataset(\n  KaggleDatasetAdapter.PANDAS,\n  \"buddycyph/train-processed\",\n  file_path,\n  # Provide any additional arguments like \n  # sql_query or pandas_kwargs. See the \n  # documenation for more information:\n  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n)\n\nprint(\"First 5 records:\", train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:23:27.429009Z","iopub.execute_input":"2025-04-06T16:23:27.429426Z","iopub.status.idle":"2025-04-06T16:23:32.596977Z","shell.execute_reply.started":"2025-04-06T16:23:27.429405Z","shell.execute_reply":"2025-04-06T16:23:32.596088Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-2-05ff9d98374a>:10: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n  train_df = kagglehub.load_dataset(\n","output_type":"stream"},{"name":"stdout","text":"First 5 records:    Score                                               Text  \\\n0      5  I received this product early from the seller!...   \n1      5  *****<br />Numi's Collection Assortment Melang...   \n2      5  I was very careful not to overcook this pasta,...   \n3      5  Buying this multi-pack I was misled by the pic...   \n4      5  These bars are so good! I loved them warmed up...   \n\n                                      Text_Processed  \n0  receive product early seller tastey great midd...  \n1  br numis collection assortment melange include...  \n2  careful overcook pasta make sure take bite eve...  \n3  buying multipack mislead picture whole hazel n...  \n4  bar good love warm definitely think great snac...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport time\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:23:06.446979Z","iopub.execute_input":"2025-04-06T16:23:06.447236Z","iopub.status.idle":"2025-04-06T16:23:27.428232Z","shell.execute_reply.started":"2025-04-06T16:23:06.447203Z","shell.execute_reply":"2025-04-06T16:23:27.427584Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def create_model(bert_name, num_classes):\n    bert = BertModel.from_pretrained(bert_name)\n    dropout = nn.Dropout(0.1)\n    classifier = nn.Linear(bert.config.hidden_size, num_classes)\n\n    def forward(input_ids, attention_mask):\n        with torch.no_grad():\n            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n        x = dropout(bert_output)\n        return classifier(x)\n\n    # Wrap into a Module for training compatibility\n    class ModelWrapper(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.bert = bert\n            self.dropout = dropout\n            self.fc = classifier\n\n        def forward(self, input_ids, attention_mask):\n            output = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n            x = self.dropout(output)\n            return self.fc(x)\n\n    return ModelWrapper()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:23:38.555430Z","iopub.execute_input":"2025-04-06T16:23:38.555762Z","iopub.status.idle":"2025-04-06T16:23:38.561660Z","shell.execute_reply.started":"2025-04-06T16:23:38.555728Z","shell.execute_reply":"2025-04-06T16:23:38.560809Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Collate function for lazy tokenization\ndef collate_batch(batch):\n    texts, labels = zip(*batch)\n    encodings = tokenizer(\n        list(texts),\n        return_tensors='pt',\n        padding='max_length',\n        truncation=True,\n        max_length=max_len\n    )\n    labels = torch.tensor([label - 1 for label in labels])  # Shift label if needed\n    return {\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask'],\n        'labels': labels\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:34:05.836937Z","iopub.execute_input":"2025-04-06T16:34:05.837276Z","iopub.status.idle":"2025-04-06T16:34:05.842081Z","shell.execute_reply.started":"2025-04-06T16:34:05.837247Z","shell.execute_reply":"2025-04-06T16:34:05.841045Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:36:32.776365Z","iopub.execute_input":"2025-04-06T16:36:32.776820Z","iopub.status.idle":"2025-04-06T16:36:32.784273Z","shell.execute_reply.started":"2025-04-06T16:36:32.776780Z","shell.execute_reply":"2025-04-06T16:36:32.783357Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()\n    predictions, targets = [], []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs, dim=1)\n            predictions.extend(preds.cpu().numpy())\n            targets.extend(labels.cpu().numpy())\n\n    return accuracy_score(targets, predictions), classification_report(targets, predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:36:53.354163Z","iopub.execute_input":"2025-04-06T16:36:53.354472Z","iopub.status.idle":"2025-04-06T16:36:53.359720Z","shell.execute_reply.started":"2025-04-06T16:36:53.354433Z","shell.execute_reply":"2025-04-06T16:36:53.358779Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def predict(text, model, tokenizer, device, max_length):\n    model.eval()\n    inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        pred = torch.argmax(outputs, dim=1)\n    return pred.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:23:45.879612Z","iopub.execute_input":"2025-04-06T16:23:45.879916Z","iopub.status.idle":"2025-04-06T16:23:45.884561Z","shell.execute_reply.started":"2025-04-06T16:23:45.879893Z","shell.execute_reply":"2025-04-06T16:23:45.883704Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"texts = train_df['Text'].tolist()\nlabels = train_df['Score'].tolist()\n\n# Hyperparameters\nbert_model_name = 'bert-base-uncased'\nnum_classes = 5\nmax_len = 128\nbatch_size = 32\nepochs = 5\nlearning_rate = 2e-5\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name)\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n\n# Prepare raw samples (lazy style)\ntrain_data = list(zip(train_texts, train_labels))\nval_data = list(zip(val_texts, val_labels))\n\n# DataLoaders\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate_batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:34:52.365118Z","iopub.execute_input":"2025-04-06T16:34:52.365401Z","iopub.status.idle":"2025-04-06T16:34:52.777451Z","shell.execute_reply.started":"2025-04-06T16:34:52.365380Z","shell.execute_reply":"2025-04-06T16:34:52.776776Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = create_model(bert_model_name, num_classes).to(device)\n\n# Optional: Use DataParallel if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    start = time.time()\n    avg_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n    print(f\"Average Training Loss: {avg_loss:.4f} | Time: {time.time() - start:.2f}s\")\n\n    acc, report = evaluate_model(model, val_loader, device)\n    print(f\"Validation Accuracy: {acc:.4f}\")\n    print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T16:36:56.096542Z","iopub.execute_input":"2025-04-06T16:36:56.096883Z","iopub.status.idle":"2025-04-06T16:37:28.253169Z","shell.execute_reply.started":"2025-04-06T16:36:56.096851Z","shell.execute_reply":"2025-04-06T16:37:28.251954Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-f0840bcfe43d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch + 1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average Training Loss: {avg_loss:.4f} | Time: {time.time() - start:.2f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-4ac190eceaa6>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0;31m# Just adding the square of the weights to the loss function is *not*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nimport time\nfrom tqdm import tqdm\n\nclass BERTClassifier(nn.Module):\n  def __init__(self, bert_model_name, num_classes):\n      super(BERTClassifier, self).__init__()\n      self.bert = BertModel.from_pretrained(bert_model_name)\n      self.dropout = nn.Dropout(0.1)\n      self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n  def forward(self, input_ids, attention_mask):\n      outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n      pooled_output = outputs.pooler_output\n      x = self.dropout(pooled_output)\n      logits = self.fc(x)\n      return logits\n  \nclass TextClassificationDataset(Dataset):\n  def __init__(self, texts, labels, tokenizer, max_length):\n          self.texts = texts\n          self.labels = labels\n          self.tokenizer = tokenizer\n          self.max_length = max_length\n  def __len__(self):\n      return len(self.texts)\n  def __getitem__(self, idx):\n      text = str(self.texts[idx])\n      label = self.labels[idx]\n      encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n      return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label-1)}\n  \ndef train(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    epoch_start_time = time.time()  # Start time for the epoch\n    for i, batch in enumerate(tqdm(data_loader, desc=\"Training\", leave=False)):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        outputs = model(input_ids, attention_mask)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Print batch progress every 100th batch\n        if i % 100 == 0:\n            print(f\"Batch: {i}, Loss: {loss.item():.4f}\")\n\n    epoch_end_time = time.time()  # End time for the epoch\n    epoch_duration = epoch_end_time - epoch_start_time\n    print(f\"Epoch time: {epoch_duration:.2f} seconds\")\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    predictions = []\n    actual_labels = []\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            predictions.extend(preds.cpu().tolist())\n            actual_labels.extend(labels.cpu().tolist())\n    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)\n\ndef predict_sentiment(text, model, tokenizer, device, max_length=128):\n    model.eval()\n    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n    return preds.item()\n    \ntexts = train_df['Text'].tolist()\nlabels = train_df['Score'].tolist()\n\nbert_model_name = 'bert-base-uncased'\nnum_classes = 5\nmax_length = 128\nbatch_size = 128\nnum_epochs = 10\nlearning_rate = 2e-5\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n#Training on the entire dataset while keeping the validation same.\ntrain_texts = texts\ntrain_labels = labels\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name)\ntrain_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\nval_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize the model and move it to the device\nmodel = BERTClassifier(bert_model_name, num_classes).to(device)\n\n# Wrap the model with DataParallel to use both GPUs\nmodel = nn.DataParallel(model)\n# If you are doing the load and train thing, use this to load:\n# model.load_state_dict(torch.load(f\"/bert{epoch}e.pt\"))\noptimizer = AdamW(model.parameters(), lr=learning_rate)\ntotal_steps = len(train_dataloader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    \n    # Training phase\n    train_start_time = time.time()\n    train(model, train_dataloader, optimizer, scheduler, device)\n    train_end_time = time.time()\n    train_duration = train_end_time - train_start_time\n    print(f\"Training time: {train_duration:.2f} seconds\")\n    \n    # Evaluation phase\n    accuracy, report = evaluate(model, val_dataloader, device)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(report)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T12:12:15.717369Z","iopub.execute_input":"2025-04-06T12:12:15.717697Z","execution_failed":"2025-04-06T13:46:12.577Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 1/2416 [00:01<1:07:09,  1.67s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 0, Loss: 1.5496\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|▍         | 101/2416 [02:50<1:05:06,  1.69s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 100, Loss: 0.6323\n","output_type":"stream"},{"name":"stderr","text":"Training:   8%|▊         | 201/2416 [05:40<1:02:46,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 200, Loss: 0.6157\n","output_type":"stream"},{"name":"stderr","text":"Training:  12%|█▏        | 301/2416 [08:31<59:58,  1.70s/it]  ","output_type":"stream"},{"name":"stdout","text":"Batch: 300, Loss: 0.6132\n","output_type":"stream"},{"name":"stderr","text":"Training:  17%|█▋        | 401/2416 [11:21<57:31,  1.71s/it]  ","output_type":"stream"},{"name":"stdout","text":"Batch: 400, Loss: 0.7654\n","output_type":"stream"},{"name":"stderr","text":"Training:  21%|██        | 501/2416 [14:11<54:09,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 500, Loss: 0.7178\n","output_type":"stream"},{"name":"stderr","text":"Training:  25%|██▍       | 601/2416 [17:01<51:59,  1.72s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 600, Loss: 0.7990\n","output_type":"stream"},{"name":"stderr","text":"Training:  29%|██▉       | 701/2416 [19:50<48:53,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 700, Loss: 0.7836\n","output_type":"stream"},{"name":"stderr","text":"Training:  33%|███▎      | 801/2416 [22:40<45:29,  1.69s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 800, Loss: 0.7393\n","output_type":"stream"},{"name":"stderr","text":"Training:  37%|███▋      | 901/2416 [25:31<43:07,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 900, Loss: 0.6151\n","output_type":"stream"},{"name":"stderr","text":"Training:  41%|████▏     | 1001/2416 [28:21<40:16,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1000, Loss: 0.6327\n","output_type":"stream"},{"name":"stderr","text":"Training:  46%|████▌     | 1101/2416 [31:11<38:12,  1.74s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1100, Loss: 0.7579\n","output_type":"stream"},{"name":"stderr","text":"Training:  50%|████▉     | 1201/2416 [34:02<34:31,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1200, Loss: 0.7761\n","output_type":"stream"},{"name":"stderr","text":"Training:  54%|█████▍    | 1301/2416 [36:53<31:40,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1300, Loss: 0.6520\n","output_type":"stream"},{"name":"stderr","text":"Training:  58%|█████▊    | 1401/2416 [39:43<28:51,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1400, Loss: 0.5906\n","output_type":"stream"},{"name":"stderr","text":"Training:  62%|██████▏   | 1501/2416 [42:33<26:10,  1.72s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1500, Loss: 0.5777\n","output_type":"stream"},{"name":"stderr","text":"Training:  66%|██████▋   | 1601/2416 [45:23<23:03,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1600, Loss: 0.5016\n","output_type":"stream"},{"name":"stderr","text":"Training:  70%|███████   | 1701/2416 [48:14<20:22,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1700, Loss: 0.5183\n","output_type":"stream"},{"name":"stderr","text":"Training:  75%|███████▍  | 1801/2416 [51:04<17:29,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1800, Loss: 0.4993\n","output_type":"stream"},{"name":"stderr","text":"Training:  79%|███████▊  | 1901/2416 [53:54<14:36,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 1900, Loss: 0.5136\n","output_type":"stream"},{"name":"stderr","text":"Training:  83%|████████▎ | 2001/2416 [56:45<11:59,  1.73s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 2000, Loss: 0.6898\n","output_type":"stream"},{"name":"stderr","text":"Training:  87%|████████▋ | 2101/2416 [59:35<08:55,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 2100, Loss: 0.6723\n","output_type":"stream"},{"name":"stderr","text":"Training:  91%|█████████ | 2201/2416 [1:02:25<06:05,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 2200, Loss: 0.6486\n","output_type":"stream"},{"name":"stderr","text":"Training:  95%|█████████▌| 2301/2416 [1:05:15<03:14,  1.69s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 2300, Loss: 0.5657\n","output_type":"stream"},{"name":"stderr","text":"Training:  99%|█████████▉| 2401/2416 [1:08:05<00:25,  1.69s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 2400, Loss: 0.5816\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Epoch time: 4109.83 seconds\nTraining time: 4109.83 seconds\nValidation Accuracy: 0.7935\n              precision    recall  f1-score   support\n\n           0       0.69      0.84      0.76      5644\n           1       0.52      0.30      0.38      3214\n           2       0.55      0.49      0.52      4679\n           3       0.58      0.39      0.47      8688\n           4       0.87      0.95      0.91     39602\n\n    accuracy                           0.79     61827\n   macro avg       0.64      0.60      0.61     61827\nweighted avg       0.77      0.79      0.78     61827\n\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 1/2416 [00:01<1:09:08,  1.72s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 0, Loss: 0.5725\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|▍         | 101/2416 [02:52<1:05:40,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 100, Loss: 0.6504\n","output_type":"stream"},{"name":"stderr","text":"Training:   8%|▊         | 201/2416 [05:42<1:02:32,  1.69s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 200, Loss: 0.5743\n","output_type":"stream"},{"name":"stderr","text":"Training:  12%|█▏        | 301/2416 [08:31<59:38,  1.69s/it]  ","output_type":"stream"},{"name":"stdout","text":"Batch: 300, Loss: 0.5608\n","output_type":"stream"},{"name":"stderr","text":"Training:  17%|█▋        | 401/2416 [11:21<57:02,  1.70s/it]  ","output_type":"stream"},{"name":"stdout","text":"Batch: 400, Loss: 0.5865\n","output_type":"stream"},{"name":"stderr","text":"Training:  21%|██        | 501/2416 [14:10<54:06,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 500, Loss: 0.5420\n","output_type":"stream"},{"name":"stderr","text":"Training:  25%|██▍       | 601/2416 [17:01<51:37,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch: 600, Loss: 0.5181\n","output_type":"stream"},{"name":"stderr","text":"Training:  28%|██▊       | 677/2416 [19:10<49:01,  1.69s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}