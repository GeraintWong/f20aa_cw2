{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11301541,"sourceType":"datasetVersion","datasetId":7067720}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies as needed:\n# pip install kagglehub[pandas-datasets]\nimport kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n\n# Set the path to the file you'd like to load\nfile_path = \"train_processed.csv\"\n\n# Load the latest version\ntrain_df = kagglehub.load_dataset(\n  KaggleDatasetAdapter.PANDAS,\n  \"jp234324/test-processed\",\n  file_path,\n  # Provide any additional arguments like \n  # sql_query or pandas_kwargs. See the \n  # documenation for more information:\n  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n)\n\nprint(\"First 5 records:\", train_df.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:35:03.336625Z","iopub.execute_input":"2025-04-07T05:35:03.336922Z","iopub.status.idle":"2025-04-07T05:35:06.098022Z","shell.execute_reply.started":"2025-04-07T05:35:03.336897Z","shell.execute_reply":"2025-04-07T05:35:06.097166Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-2-ace5bdba9b40>:10: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n  train_df = kagglehub.load_dataset(\n","output_type":"stream"},{"name":"stdout","text":"First 5 records:    Score                                               Text  \\\n0      5  I received this product early from the seller!...   \n1      5  *****<br />Numi's Collection Assortment Melang...   \n2      5  I was very careful not to overcook this pasta,...   \n3      5  Buying this multi-pack I was misled by the pic...   \n4      5  These bars are so good! I loved them warmed up...   \n\n                                      Text_Processed  \n0  receive product early seller tastey great midd...  \n1  br numis collection assortment melange include...  \n2  careful overcook pasta make sure take bite eve...  \n3  buying multipack mislead picture whole hazel n...  \n4  bar good love warm definitely think great snac...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import DistilBertTokenizer, DistilBertModel, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport time\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:35:06.098898Z","iopub.execute_input":"2025-04-07T05:35:06.099116Z","iopub.status.idle":"2025-04-07T05:35:37.024674Z","shell.execute_reply.started":"2025-04-07T05:35:06.099086Z","shell.execute_reply":"2025-04-07T05:35:37.024029Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def create_model(bert_name, num_classes):\n    bert = DistilBertModel.from_pretrained(bert_name)\n    dropout = nn.Dropout(0.1)\n    classifier = nn.Linear(bert.config.hidden_size, num_classes)\n\n    def forward(input_ids, attention_mask):\n        with torch.no_grad():\n            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n        x = dropout(bert_output)\n        return classifier(x)\n\n    # Wrap into a Module for training compatibility\n    class ModelWrapper(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.bert = bert\n            self.dropout = dropout\n            self.fc = classifier\n\n        def forward(self, input_ids, attention_mask):\n            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            pooled_output = bert_output.last_hidden_state[:, 0] \n            x = self.dropout(pooled_output)  \n            return self.fc(x)  \n\n    return ModelWrapper()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:35:37.025997Z","iopub.execute_input":"2025-04-07T05:35:37.026497Z","iopub.status.idle":"2025-04-07T05:35:37.031990Z","shell.execute_reply.started":"2025-04-07T05:35:37.026475Z","shell.execute_reply":"2025-04-07T05:35:37.030956Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Collate function for lazy tokenization\ndef collate_batch(batch):\n    texts, labels = zip(*batch)\n    encodings = tokenizer(\n        list(texts),\n        return_tensors='pt',\n        padding='max_length',\n        truncation=True,\n        max_length=max_len\n    )\n    labels = torch.tensor([label - 1 for label in labels])  # Shift label if needed\n    return {\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask'],\n        'labels': labels\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:14.984463Z","iopub.execute_input":"2025-04-07T06:24:14.984809Z","iopub.status.idle":"2025-04-07T06:24:14.989645Z","shell.execute_reply.started":"2025-04-07T06:24:14.984780Z","shell.execute_reply":"2025-04-07T06:24:14.988765Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:40:14.913147Z","iopub.execute_input":"2025-04-07T05:40:14.913509Z","iopub.status.idle":"2025-04-07T05:40:14.919085Z","shell.execute_reply.started":"2025-04-07T05:40:14.913481Z","shell.execute_reply":"2025-04-07T05:40:14.918045Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()\n    predictions, targets = [], []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs, dim=1)\n            predictions.extend(preds.cpu().numpy())\n            targets.extend(labels.cpu().numpy())\n\n    return accuracy_score(targets, predictions), classification_report(targets, predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:35:37.087892Z","iopub.execute_input":"2025-04-07T05:35:37.088122Z","iopub.status.idle":"2025-04-07T05:35:37.106555Z","shell.execute_reply.started":"2025-04-07T05:35:37.088105Z","shell.execute_reply":"2025-04-07T05:35:37.105981Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def predict(text, model, tokenizer, device, max_length):\n    model.eval()\n    inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        pred = torch.argmax(outputs, dim=1)\n    return pred.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T05:35:37.107259Z","iopub.execute_input":"2025-04-07T05:35:37.107473Z","iopub.status.idle":"2025-04-07T05:35:37.122153Z","shell.execute_reply.started":"2025-04-07T05:35:37.107454Z","shell.execute_reply":"2025-04-07T05:35:37.121584Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"texts = train_df['Text_Processed'].tolist()\nlabels = train_df['Score'].tolist()\n\n# Hyperparameters\nbert_model_name = 'distilbert-base-uncased'\nnum_classes = 5\nmax_len = 128\nbatch_size = 128\nepochs = 10\nlearning_rate = 2e-5\n\ntokenizer = DistilBertTokenizer.from_pretrained(bert_model_name)\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\ntrain_texts = texts\ntrain_labels = labels\n\n\ntrain_data = list(zip(train_texts, train_labels))\nval_data = list(zip(val_texts, val_labels))\n\n# DataLoaders\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=collate_batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:19.919027Z","iopub.execute_input":"2025-04-07T06:24:19.919393Z","iopub.status.idle":"2025-04-07T06:24:20.302356Z","shell.execute_reply.started":"2025-04-07T06:24:19.919360Z","shell.execute_reply":"2025-04-07T06:24:20.301641Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = create_model(bert_model_name, num_classes).to(device)\n\n# Optional: Use DataParallel if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\ntotal_steps = len(train_loader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    start = time.time()\n    avg_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n    print(f\"Average Training Loss: {avg_loss:.4f} | Time: {time.time() - start:.2f}s\")\n\n    acc, report = evaluate_model(model, val_loader, device)\n    print(f\"Validation Accuracy: {acc:.4f}\")\n    print(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T06:24:22.608091Z","iopub.execute_input":"2025-04-07T06:24:22.608410Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.7763 | Time: 2133.14s\nValidation Accuracy: 0.7474\n              precision    recall  f1-score   support\n\n           0       0.66      0.72      0.69      5644\n           1       0.45      0.14      0.21      3214\n           2       0.42      0.40      0.41      4679\n           3       0.51      0.28      0.36      8688\n           4       0.82      0.95      0.88     39602\n\n    accuracy                           0.75     61827\n   macro avg       0.57      0.50      0.51     61827\nweighted avg       0.71      0.75      0.72     61827\n\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.6802 | Time: 2129.33s\nValidation Accuracy: 0.7684\n              precision    recall  f1-score   support\n\n           0       0.66      0.82      0.73      5644\n           1       0.50      0.22      0.31      3214\n           2       0.49      0.43      0.46      4679\n           3       0.63      0.20      0.30      8688\n           4       0.83      0.97      0.89     39602\n\n    accuracy                           0.77     61827\n   macro avg       0.62      0.53      0.54     61827\nweighted avg       0.74      0.77      0.73     61827\n\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.6259 | Time: 2128.28s\nValidation Accuracy: 0.7952\n              precision    recall  f1-score   support\n\n           0       0.73      0.82      0.77      5644\n           1       0.54      0.30      0.39      3214\n           2       0.54      0.52      0.53      4679\n           3       0.62      0.35      0.44      8688\n           4       0.86      0.96      0.91     39602\n\n    accuracy                           0.80     61827\n   macro avg       0.66      0.59      0.61     61827\nweighted avg       0.77      0.80      0.78     61827\n\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.5757 | Time: 2126.64s\nValidation Accuracy: 0.8125\n              precision    recall  f1-score   support\n\n           0       0.73      0.88      0.80      5644\n           1       0.59      0.32      0.42      3214\n           2       0.58      0.61      0.59      4679\n           3       0.73      0.31      0.43      8688\n           4       0.87      0.98      0.92     39602\n\n    accuracy                           0.81     61827\n   macro avg       0.70      0.62      0.63     61827\nweighted avg       0.80      0.81      0.79     61827\n\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.5292 | Time: 2125.68s\nValidation Accuracy: 0.8409\n              precision    recall  f1-score   support\n\n           0       0.81      0.87      0.84      5644\n           1       0.64      0.48      0.55      3214\n           2       0.62      0.71      0.66      4679\n           3       0.71      0.46      0.56      8688\n           4       0.90      0.96      0.93     39602\n\n    accuracy                           0.84     61827\n   macro avg       0.74      0.70      0.71     61827\nweighted avg       0.83      0.84      0.83     61827\n\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.4874 | Time: 2123.49s\nValidation Accuracy: 0.8623\n              precision    recall  f1-score   support\n\n           0       0.85      0.87      0.86      5644\n           1       0.67      0.60      0.64      3214\n           2       0.68      0.74      0.71      4679\n           3       0.77      0.52      0.62      8688\n           4       0.91      0.97      0.94     39602\n\n    accuracy                           0.86     61827\n   macro avg       0.78      0.74      0.75     61827\nweighted avg       0.86      0.86      0.85     61827\n\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.4488 | Time: 2122.63s\nValidation Accuracy: 0.8773\n              precision    recall  f1-score   support\n\n           0       0.88      0.89      0.89      5644\n           1       0.73      0.64      0.68      3214\n           2       0.72      0.79      0.76      4679\n           3       0.86      0.50      0.63      8688\n           4       0.91      0.99      0.95     39602\n\n    accuracy                           0.88     61827\n   macro avg       0.82      0.76      0.78     61827\nweighted avg       0.87      0.88      0.87     61827\n\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.4178 | Time: 2125.21s\nValidation Accuracy: 0.8920\n              precision    recall  f1-score   support\n\n           0       0.86      0.93      0.89      5644\n           1       0.76      0.65      0.70      3214\n           2       0.77      0.80      0.78      4679\n           3       0.87      0.58      0.69      8688\n           4       0.92      0.99      0.95     39602\n\n    accuracy                           0.89     61827\n   macro avg       0.84      0.79      0.80     61827\nweighted avg       0.89      0.89      0.89     61827\n\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  20%|█▉        | 476/2416 [06:58<28:31,  1.13it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}